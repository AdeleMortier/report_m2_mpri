\documentclass[11pt]{article}
\usepackage[utf8]{inputenc} %
\usepackage{geometry}
\geometry{
	a4paper,
	left=20mm,
	top=20mm,
	bottom=25mm,
	right=20mm
	}
\usepackage{qtree}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage[nointegrals]{wasysym}
\usepackage{mathtools}
\usepackage{soul}
\usepackage{ulem}


\usepackage[title]{appendix}


\newenvironment{sbcquote}[2]
{
	\def\savedcaption{\caption{#1}}%
	\def\savedlabel{\label{#2}}%
	\begin{table}[h]
		\centering
		\begin{small}
		\begin{tabular}{ll}
			\hline
			& \\
}
{
	& \\
	\hline
	\end{tabular}
	\end{small}
	\savedcaption
	\savedlabel
\end{table}
}

\newenvironment{recenv}
{\left[\begin{array}{lll}}
{\end{array}\right]}


\begin{document}
	\title{Incremental dialogue modeling -- internship report}
	\author{\begin{tabular}{rcl}
			Auteure &:& AdÃ¨le Mortier \\
			Encadrant &:& Jonathan Ginzburg\\
			Laboratoire &:& LLF Paris 7
		\end{tabular}
	}
	\maketitle
	
	

	\section*{General Introduction}
		Dialogue modeling is a subfield of computational linguistics that intends to tackle natural language using a wider perspective. While natural language semantics usually focuses on single sentences, dialogue semantics addresses the problem of long run interactions between several speakers. This crucially requires to define adequate and more sophisticated structures that could keep track of the many aspects of the ongoing dialogue: what has been asserted so far, what questions remain unanswered, what kind of contextual elements are involved etc. Such a fine grained analysis should of course be based on previous work in semantic modeling, but should also adopt a more holistic viewpoint. That is where the use of structures from the field of computer science appears to be beneficial.\\
		
		Another challenge beyond information storage and modeling relates to the cognitive processes that underlie real-world, multi-speaker interactions. Indeed, a realistic conversation is seldom totally fluid: people hesitate, do (self-)repair, produce disfluencies or tend to digress. All these phenomena have only been recently taken into account, and seem difficult to model in a systematic, computer-oriented way. We have tried to consider a model for incremental speech processing that could both emulate effective cognitive processes and compute information in a more flexible, reversible way. Applications of this models could be found in modern dialogue management systems, where a human speaker interacts with a on-human entity (for instance the TRAINS project\footnote{\href{https://www.cs.rochester.edu/research/cisd/resources/trains.html}{https://www.cs.rochester.edu/research/cisd/resources/trains.html}}).
		
	\section{The nature of dialogue}
		We begin by a brief review of the notion of dialogue. What is a dialogue? The notion seems to be rather straightforward in the sense that it is deeply rooted in our use of language. We learn a language by listening to dialogues, and dialogue is our principal means of expression, leaving aside the literary language that we use in written communication, for instance in this report. However, the knowledge that we have about dialogue \textit{instances} is not a knowledge about dialogue in itself. Here we will try to clarify what we mean by dialogue; we will introduce its main features through corpus examples\footnote{we have used for this purpose the Santa Barbara corpus of spoken American English \cite{dubois2000}, which is based on a large body of recordings of naturally occurring spoken interaction from all over the United States. The Santa Barbara Corpus represents a wide variety of people of different regional origins, ages, occupations, genders, and ethnic and social backgrounds. The predominant form of language use represented is face-to-face conversation (source: \href{http://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{http://www.linguistics.ucsb.edu/research/santa-barbara-corpus}). Each excerpt we picked goes with the name of the file it comes from within the corpus. A more precise description of the context is given for each file in the webpage previously mentioned.}; and we will give a sketch of the underlying linguistic background (syntax, semantics).
		\subsection{What is dialogue? First explanations and examples}
			The word dialogue is opposed to monologue; indeed, a dialogue most evidently involves two or more participants that are supposed to speak in turn.\\
			
			In Table \ref{tab:dial_turn_taking} for instance, the dialogue involves three speakers, who perform $2$ speech acts each. But of course, the boundaries between speech acts are not always clear, when people tend to interrupt each other, or simply when the turn-taking is triggered in the middle of an utterance \footnote{as we can see in the first intervention of LYNNE}. Thus, a dialogue is not necessarily sequential, and does not necessarily include well formed sentences. It involves a kind of concurrency between the participants.
			
			\begin{sbcquote}{dialogue as a turn-taking process (SBC001.trn)}{tab:dial_turn_taking}
				LENORE: & [Is !Amanda] her kid,\\
				& or, \\
				LYNNE: & .. Yeah.\\
				LENORE: & [Oh].\\
				DORIS: & [she had] a little three-year-old daughter.\\
				LYNNE: & ... It's just too bad.\\
				& ... I just hope !Orville and !Genetta get that little girl.\\
				DORIS: & Oh they won't.\\
			\end{sbcquote}


			Besides its superficial structure, a dialogue is also a series of motivated acts: if we talk to each other, it is not usually for the sake of talking. We speak, because we share a common goal; we want to exchange informations. The participants usually share a certain amount of information, which is considered as the common ground (for instance, that the earth is flat, that human beings live about a hundred years...). But they also have personal ideas they can share using informative sentences \footnote{most of the time, the notion coincides with declarative sentences}. Conversely, information can consists of queries using inquisitive sentences \footnote{most of the time, the notion coincides with interrogative sentences}.\\
			
			All these actions, all these speech acts, have a direct influence on the common ground, that is continuously updated during the interaction. Conversation is hence a continuous effort of coordination between the participants that try to align themselves to the common ground in real time \cite{poesio1997, eshghi2015}. This is crucial for the dialogue not to turn into a monologue.\\

			An example of such a coordination mechanism is given in Table \ref{tab:dial_grounding}. First, the fact that ``he'' is a nice guy should be added to the common ground (we assume implicit acknowledgement). After that, the fact that Ron wonders whether ``he'' teaches in schools should be tracked in some way. Then, the fact that ``he'' is not married and gives private lessons and orchestras should be grounded as well (here the acknowledgement is made explicit).\\
			
			\begin{sbcquote}{dialogue and the duality between informativeness and inquisitiveness (SBC019.trn)}{tab:dial_grounding}
				FRANK: &... (TSK) He seems like a prety nice guy though.\\
				RON: & Does he teach in the school system too?\\
				& Or --\\
				FRANK: & M-m.\\
				& ... (TSK) He's strictly,\\
				& ... pri- --\\
				& he's not married or anything,\\
				& and I think strictly private lessons,\\
				& a=nd orchestra.\\
				RON: & Hm.\\
			\end{sbcquote}

			

			Finally, information can be conveyed by non-linguistic elements, like perceptions of the environment, and gestures. The processing of these informations, that include multimodal channels, is particularly challenging (cf. Table \ref{tab:dial_env1} and Table \ref{tab:dial_env2}).

			\begin{sbcquote}{dialogue and the environment (SBC038.trn)}{tab:dial_env1}
				BEN: &... Anybody needs the elevator it's available.\\
				$>$ENV: & ((CROWD\_AND\_MACHINE\_NOISE))\\
				BEN: & Anybody that needs the elevator it's $$<$$X is X$>$ available.\\
				$>$ENV: & ((CROWD\_AND\_MACHINE\_NOISE))\\
				BEN: & ... Come on.\\
				$>$ENV: & ((CROWD\_AND\_MACHINE\_NOISE))\\
			\end{sbcquote}
			
			\begin{sbcquote}{dialogue and the environment (SBC040.trn)}{tab:dial_env2}
				DEBRA: & (H) he is twenty-four years old,\\
				& and stands a very impressive (H) seventeen point one hands tall.\\
				&... Ladies and gentlemen,\\
				&the great handicap horse,\\
				& ... Forego.\\
				MANY: & ... ((APPLAUSE))\\
			\end{sbcquote}

			Therefore, dialogue has a specific structure that relies on turn-taking, and involves complex exchange of informations. In our study, we will thus assume some simplifications :
			\begin{itemize}
				\item we will focus on \textit{oral dialogue} (as opposed to the written dialogues that can be found on forums etc.); \vspace{-2mm}
				\item we will only consider \textit{two speakers}; \vspace{-2mm}
				\item we will not deal with simultaneous utterances, only \textit{sequential} ones.
			\end{itemize}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%                     GROUNDING                      % poesio
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
		\subsection{How is dialogue processed?}
			In this section we will argue for the inherent incrementality of dialogue, and justify why it should be processed in that way too.
			\subsubsection{Cognitive realism}
				First, it is obvious that dialogue is produced incrementally: when people speak together, they cannot have a precise idea of what will be said in the long run. What people say depend on what they think of course, but also on what they think the addressee will think... and so forth \cite{frank2016}. We have also seen that conversations can involve asymmetries of information when the two participants are not ``synchronized'' with respect to the common ground. They thus produce utterances -- if not even words -- on the fly. This incremental production mechanism is evidenced by so-called \textit{disfluencies} or self-corrections (cf. Table \ref{tab:dial_grounding}, FRANK $2^{nd}$ turn).\\
				
				Second, there is a large amount of psycholinguistic evidence\footnote{Experimentally, eye-tracking techniques have shown that reference resolution begins right after the first hint is given and long before all the ambiguity is cleared up \cite{tanenhaus1996}. In particular it has also been shown that the processing of verbs introduces bias towards the most probable type of object \cite{altmann1999}. \cite{traxler1997} showed that interclausal relationships (causation, diagnostic) are processed before the end of the last clause. Using another technique, namely event-related brain potentials (ERP), \cite{schlesewsky2004} advocate for the existence of two different interpretation pathways (a syntactic one and a thematic one) that however do not achieve the same degree of incrementality. But for reasons of space we cannot dive deeply into these topics.} showing that people understand sentences incrementally. In other words, people begin to form and compose semantic objects that correspond to the word they read or hear in one shot; and they are even able to anticipate the form/meaning of the subsequent entities. This of course can lead to syntactic misunderstanding and backtrackings, as showed by the so-called ``garden-path sentences'' \cite{sag2015}:
				\begin{center}
					``The horse raced past the barn fell.'' \cite{bever2013}\vspace{1mm}\\
					Intuitive first reading: $[$The horse$]_{NP}$ $[ [$raced$]_V$ $[$past the barn$] ]_{VP}$ $[$fell$]_?$. \vspace{1mm}\\
					More consistent second reading: $[[$The horse$]_{NP}$ $[$raced past the barn$]]_{NP}$ $[$fell$]_{VP}$.\\
				\end{center}
				Other examples involving lexical ambiguity can be found in \cite{sag2015}, for instance:
				\begin{center}
					The pen$_{area?, tool?}$ is in the box \\
					The box is in the pen$_area$
				\end{center}
				In the first sentence, the ambiguity regarding the meaning of ``pen'' remains until the end of the sentence. Yet we are able to understand this sentence quite rapidly, most often without backtracking. This kind of phenomenon advocates for the fact that we do not actually process all the possible alternatives in parallel and ``prune the tree'' afterwards. We rather keep in mind and underspecified version of the lexeme (here, ``pen''), and enrich only when we get additional informations about the most probable meaning (here, the word ``box'' allows to opt for pen$_tool$). Thus as we will see, underspecification is a strong desideratum of incrementality.\\
				
				
				
				
				%\cite{levelt1993} introduces a partition between the different components of a speech act: the conceptualizer generates preverbal messages in the mind of the speaker, the formulator translates them into grammatical sentences, the articulator executes the phonetic plan that underlies the sentences, and finally, the speech-comprehension system ensure the translation from overt sentences to conceptual entities.\\
				
				An automatic dialogue system that could achieve incrementality would thus be in the position to achieve cognitive realism as well as speech naturalness, as pointed out by \cite{schlangen2009}. Indeed, incrementality goes with a following valuable features, which is sub-sentential intervention, that is closely related to efficiency as well.

			\subsubsection{Computational efficiency}
				On the other hand, incremental processing may prove more effective than usual holistic parsing methods. First, the system would certainly gain in time efficiency, since incremental processing can begin as soon as the first word (or more generally, the first \textit{incremental unit}, cf. \ref{iu_model}) is produced. Thus, even if the incremental computation takes a longer (but reasonable) time, it can still be advantageous.\\
				
				Second, an incremental system should be able to converse with the user more efficiently, \textit{i.e.} avoid useless explanations or conversational dead-ends. A perfect incremental system should indeed process the maximal amount of information at each step (in a word-by-word basis), including for instance anaphora resolution. This feature, called \textit{strong incremental interpretation} is notably pointed out by \cite{hough2015}, after \cite{milward1991}.  An even more desirable system should be in a position to predict with a relatively high confidence what will come \textit{next}. Such an advanced ability would allow for ellipsis, sentence completion and ``just-in-time'' correction; more precisely, this could be used to:
				\begin{itemize}
					\item show early agreement, and thus possibly avoid further unnecessary explanations (cf. Table \ref{tab:dial_sent_compl2}); \vspace{-2mm}
				 	\item underline an inconsistency to trigger justifications or self-correction, thus avoiding a propagation of the mistake (cf. Table \ref{tab:dial_echoing_corr}); \vspace{-2mm}
				 	\item complete a sentence when its meaning appears sufficiently predictable, and thus save the speaker's time and effort (cf. Table \ref{tab:dial_sent_compl2} and Table \ref{tab:dial_sent_compl1}).
				\end{itemize}
				
				\begin{sbcquote}{sentence completion and early agreement (SBC048.trn)}{tab:dial_sent_compl2}
					JUDY:&	Well Dad always [4picks4],\\
					TIM:&	[4I4] .. get a little picky o=n ... my shirts (Hx).\\
					JUDY:&	.. @(Hx) ... Yeah,\\
					&I know.\\
				\end{sbcquote}
				
				\begin{sbcquote}{echoing and sentence correction (SBC012.trn)}{tab:dial_echoing_corr}
					GILBERT: &.. I don't think [2there's as2] much [3trust3],\\
					MONTOYA:& [2Oh2],\\
					& [3Perot3].\\
					GILBERT:& 	.. XX,\\
					MONTOYA:& 	Trust.\\
					GILBERT:& 	Trust in the system.\\
					MONTOYA:& 	.. [Why].\\
				\end{sbcquote}
				
				\begin{sbcquote}{sentence completion (SBC048.trn)}{tab:dial_sent_compl1}
					JUDY:&	Oh,\\
					&this is great.\\
					&... Stainless steel?\\
					&... Hmm?\\
					TIM:&	[Yep].\\
					JUDY:&	[Stai-] --\\
					LEA:&	[Stain]less.\\
					&.. Unhunh.\\
				\end{sbcquote}
			
			In the following section we will introduce very briefly the current challenges of formal syntax and semantics and how this relates to theoretical computer science. We will set aside lower levels of linguistics (phonology, morphology), because they do not directly concern our main problem.
		\subsection{A bird's eye view on sentence-level linguistics}
			Here we give a very brief account of the formal, computer-science oriented treatment of linguistics. The ideas that follow are from the most mainstream theories and representations of the field; that is why we will not necessarily stick to them in the following sections.
			\subsubsection{Syntax}
				The formal syntax of natural language relies on the notion of tree. The syntactic tree can be understood in two ways that are equivalent:
				\begin{itemize}
					\item the top-down approach: the whole sentence is divided into sub-constituents called phrases (verb phrase, noun phrase...), and each of these constituents are divided again until a so-called head is produced. The head (verb, noun, determiner, preposition) has a unique child, which is a lexical item (a word). The top-down approach is often related to language generation;
					\item the bottom-up approach: the words of the sentence are mapped to part-of speech tag (the heads), which are in turn merged to form phrases. The merge operation stops when the root (the whole sentence) is produced. The bottom-up approach is often related to syntactic parsing;
				\end{itemize}
				However, it is important to note that the merge strategy (or equivalently the branching strategy) is not fully deterministic; sometimes several rules can be applied and thus the same sentence can lead to different parse trees. This is exemplified in Figure \ref{fig:att_pb}, where the constituent ``with a telescope'' can be seen as a complement of the verb or as a complement of the NP ``a man''. This syntactic (structural) ambiguity generates a semantic ambiguity. Therefore, the parsing strategy is considered as crucial in an incremental dialogue system, that is expected to process the words in one pass.
				
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				%%        Traditional incremental parsing		  %%
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				
	
			\subsubsection{Semantics}
				It is generally assumed that the semantics of a sentence computed by applying a kind of morphism to its syntactic structure. Indeed, this idea complies with the so-called principle of compositionality \cite{frege1997}, according to which the semantics of a sentence depends on the semantics of its smallest constituents, and on the way they combine.\\
				
				Thus, the computation of the semantics of a sentence amounts to decorating the syntactic tree with semantic expressions. Semantic expressions are basically:
				\begin{itemize}
					\item truth values of type \texttt{t} [base case];\vspace{-2mm}
					\item functions from entities (type \texttt{e}) to semantic expressions [inductive case].
				\end{itemize} 
				The ``merge'' operation that is used to create the syntactic tree is also mapped to a particular operation in the semantic domain, which is, in the most simple framework, functional application. Note that the last merge operation must generate a term of type t, because -- in truth-conditional semantics at least -- sentences must have a certain truth value.
				\begin{figure}[h]
					\Tree
					[.{\textbf{likes}(\textbf{apples}, \textbf{John}) : t} [.{\textbf{John} : e} John ] [.{$\lambda$ x. \textbf{likes}(\textbf{apples}, x) : e $\rightarrow$ t} [.{$\lambda$ y $\lambda$ x. \textbf{likes}(y, x) : e $\rightarrow$ e $\rightarrow$ t} likes ] [.{\textbf{apples} : e} apples ] ] ]
					\caption{Simple semantic computation (bold text for semantic entities)}
					\label{fig:simple_sem_tree}
				\end{figure}
				Again, the semantic layer can be subject to certain types of ambiguity, in particular with regards to quantifier scope \cite{milward1994}:
				\begin{center}
					``Every man loves a woman'' \\
					Universal reading : $[\forall x : \textbf{man}(x)] ( [\exists y : \textbf{woman}(y)] (\textbf{love}(x, y)))$\\
					Existential reading : $[\exists y : \textbf{woman}(y)]([\forall x : \textbf{man}(x)](\textbf{love}(x, y)))$
				\end{center}
				
				
				As we will see later on (in Section \ref{RMRS} in particular), this can be ``resolved'' -- or at least, accounted for -- through underspecification

	\section{A model for dialogue}\label{dialogue_model}
		We present here a framework for dialogue modeling, that unifies and brings together ideas from various schools of thought in formal semantics. The core of the theory is strongly inspired by Head-driven Phrase Structure Grammar (HPSG) \cite{sag2003} and Type Theory with Records (TTR) \cite{betarte98, cooper2005}. For the purpose of \textit{dialogue} modeling, elements of situation semantics \cite{barwise1998} are also taken into account.
		
		
		%Finally, the incremental aspects of syntactic and semantic processing are taken from existing probabilistic parsing methods \cite{ytrestol2011,sagae2007,miyao2008} and dynamic semantics \cite{kamp2011,hough2017,milward1994}. The aim of this approach is to provide a realistic, fine-grained account of multiple language phenomena that occur during everyday dialogues. Thus, emphasis will be placed on incrementality and interactions between speaker and listener, through clarification, repair, acknowledgement, contradiction. 

		\subsection{Type theory with records and HPSG}
			\subsubsection{Type theory with record (TTR)}
				Our framework for dialogue modeling is based on Type Theory with Records (TTR) \cite{betarte98}, which allows to use relatively elaborate data structures whose general form is :
				\begin{eqnarray}
					R = \left[
					\begin{array}{l l l}
					l_1 &=& k_1 \\
					l_2 &=& k_2(l_1) \\
					&\dots& \\
					l_i &=& k_i(l_1, \dots l_{i-1}) \\
					&\dots& \\
					l_n &=& k_n(l_1, \dots l_{n-1})
					\end{array}
					\right]
					&\mbox{where}&
					R \ : \left[
					\begin{array}{l l l}
					l_1 &:& T_1 \\
					l_2 &:& T_2(T_1) \\
					&\dots& \\
					l_i &:& T_i(T_1, \dots T_{i-1}) \\
					&\dots& \\
					l_n &:& T_n(T_1, \dots T_{n-1})
					\end{array}
					\right]
					\label{record_def}
				\end{eqnarray}
				These are basically ordered tuples with labels $(l_i)_{i\in[1, n]}$, values $(k_i)_{i\in[1, n]}$, dependent types $(T_i)_{i\in[1, n]}$, and a simple rule for typing :
				\begin{equation*}
					\forall i \in [1, n], \ k_i : T_i(T_1, \dots T_{i-1})
					\label{record_typing}
				\end{equation*}
				Note however that the relation $:$ is a kind of maximal typing relation. Any subset of fields extracted from the maximal record type can also be considered as a type for R, provided that the pairwise typing relation (restricted to the remaining fields) is still verified. More precisely:
				\begin{equation*}
					R : T \iff \left\lbrace\begin{array}{l}
					\forall l \in labels(T), \ l \in labels(R) \\
					\forall l :_T T' \in labels(T), \ R.l : T'
					\end{array}\right.
				\end{equation*}
				Where:
				\begin{itemize}
					\item $labels(X)$ is the set of labels that are present ix $X$, $X$ being a record or a record type;\vspace{-2mm}
					\item $R.l$ is the value of the field labeled by $l$ in the record $R$. This is a classic ``path'' notation.
				\end{itemize}
				This more permissive typing allows in fact for subtyping and polymorphism \cite{cooper2005}.\\
				
				TTR also allows for manifest types, which are of the form:
				\begin{equation*}
					l_{=k} : T
				\end{equation*}
				This means that the label $l$ is typed by a type $T$ that has only one inhabitant. This unique inhabitant is written in the subscript of the label (here, it is named $k$). This may be seen as a shorter way to type and declare a variable at the same time.
			\subsubsection{Head-driven Phrase Structure Grammar (HPSG)}
				Head-driven Phrase Structure Grammar (HPSG) \cite{sag2003} is a descendant of Generalized Phrase Structure Grammar. It is an integrated formalism for linguistic modeling. HPSG entities are highly lexicalized \textit{feature structures} inspired from the Saussurian concept of \textit{sign}. A sign is -- in this sense -- the union of a \textit{signifier} (phonic component) and a \textit{signified} (syntactic category, semantic content). Feature structures indeed pack all these elements together, in the form of what could be seen as records. A very simplified example of an HPSG entry can be found in Figure \ref{fig:hpsg_verb_entry}\footnote{Note that in this original presentation, types do not appear as a central element, in the sense that they are not systematically specified (they appear as ``headers'' of the feature structures). However the theory actually takes these types into account: types feature structures are indeed constrained by multiple-inheritance trees.}.\\
				\begin{figure}[h]
					\centering
					$\begin{recenv}
						verb-lxm && \\
						SYN && \begin{recenv}
									HEAD
									\begin{recenv}
										verb && \\
										PRED && -\\
										INF && /\footnote-\\
										AUX && /-\\
										POL && -\\
									\end{recenv}
								\end{recenv}\\
						SEM && \begin{recenv}
							MODE && prop\\
								\end{recenv}\\
						ARG-ST && \left \langle
							\begin{recenv}
								HEAD && nominal \\VAL && \begin{recenv}
									SPR && \langle \ \rangle \\C
									OMPS && \langle  \ \rangle
								\end{recenv}
							\end{recenv}, \dots
						 \right \rangle
					\end{recenv}$
					\caption{Example of an HPSG entry (feature structure) for a verb \cite{sag2003}}
					\label{fig:hpsg_verb_entry}
				\end{figure}
				\footnotetext{the symbol ``/'' is used in HPSG to indicate that a certain specification is ``defeasible'' in the sens that it can be overridden by a conflicting specification \cite{sag2003}.}
				
				HPSG is a constraint-based approach, which thus involve unification processes. In brief, three kinds of syntactic rules apply to HPSG feature structures:
				\begin{itemize}
					\item phrase-structural schemata (inheritance relationships);\vspace{-2mm}
					\item feature percolation principles (related to headedness, syntactic merge and unification);\vspace{-2mm}
					\item principles accounting for major syntactic phenomena (binding theory...).
				\end{itemize}
				Like HPSG entries, HPSG rules are modeled as records with two fields (one for the input, one for the output). This formulation will be used later to model conversational rules in Section \ref{conversational_rules}.\\
				
				HPSG \textit{per se} is not totally well suited for incremental processing, however we will see that existing meta-languages (especially MRS, see Section \ref{RMRS}) or variants of this formalism (SBCG \cite{sag2015}...) allow for more flexibility, in particular with respect to underspecification.
			
		\subsection{Situation semantics}
			Situation semantics is a framework that has been developed by Jon Barwise and John Perry in the 90's \cite{barwise1998}, and which takes ideas from the Austinean theory of truth and speech acts. For Austin, ``A statement is made and its making is an historic event, the utterance by a certain speaker or writer of certain words (a sentence) to an audience with reference to an historic situation, event or what not.''. For a statement to be true, the \textit{descriptive conventions} (that assign situation types to words) must be correlated with the \textit{demonstrative conventions} (that assign to each statement some ``historic'' situations in the world).\\
			
			This idea that truth is a relation between situations and situation types has been picked up by Barwise and Perry in more formal terms. A good introductory account of this can be found in \cite{stojanovic2011}. Situation semantics, unlike the various derivatives of fregean semantics, focuses on the notion of \textit{situation} (and not truth values, or ``Sinn''...). Truth and falsity are just treated as side-effects of the theory.\\
			
			So, in situation semantics, the meaning of an utterance is not equivalent to its truth conditions, it is rather seen as a relation between situations: the situation related to the speech act, and the situation related to what is said. A situation can then be seen as an underspecified world (in the sense of possible world semantics), where some constraints are met, but some other properties remain unknown. Situations can easily be modeled as records. For instance, a situation where John loves Mary could be modeled as:
			\begin{equation*}
				\begin{recenv}
					j &:& \texttt{Ind} \\
					m &:& \texttt{Ind} \\
					s &:& \texttt{love}(j, m)
				\end{recenv}
			\end{equation*}
			More precisely, the meaning of an utterance u (i.e. a string of words) about an event e is modeled as:
			\begin{equation*}
			 s_u \llbracket u \rrbracket s_e
			\end{equation*}
			Where $\llbracket u \rrbracket$ specifies a certain number of constraints linking the fields of $s_u$ and $s_e$. In TTR, these constraints can easily be expressed as typing constraints, for instance \cite{barwise1998}:
			\begin{eqnarray*}
				s_u \llbracket\mbox{ I am sitting } \rrbracket s_e
				&\iff&
				s_u : \begin{recenv}
					i &:& \texttt{Ind} \\
					l &:& \texttt{Loc}\\
					s &:& \texttt{speak}(i, l)
				\end{recenv} \bigwedge
				s_e : \begin{recenv}
					i &:& \texttt{Ind} \\
					l &:& \texttt{Loc}\\
					s &:& \texttt{sit}(i, l)
				\end{recenv}
				\bigwedge					
				\left\lbrace\begin{array}{l}
				s_u.i = s_e.i\\
				s_u.l = s_e.l
				\end{array}\right.
			\end{eqnarray*}
			
			%=> semantics focused on the notion of situation. contra fregean semantics that is focused on sinn (like objects in a third realm) and truth conditional semantics (true or false as references). Here, straightforward treatment of counterfactuals.Truth is a side effect of the theory.=> meaning is interpreted as a relation between situations. I mean u when I want to describe a situation where u is true and when i am in a situation where i say that u. described situation vs utterance situation	=> underspecification. We don't know for each p, wheter p or not p.	=> relation : good for the treatment of indexicals, perspective-related words...
%			%common ground = discourse situation
			
		\subsection{Dialogue GameBoards}
			The formal features of dialogue are highly inspired from \cite{ginzburg2012,ginzburg2017}, although some of them are slightly modified.
			Records and record types will be used to model the various (sometimes heterogeneous) features of a dialogue:
			\begin{itemize}
				\item turn (who speaks and who listens); \vspace{-2mm}
				\item ``doxastic'' states (facts people believe); \vspace{-2mm}
				\item environment (in particular visual informations); \vspace{-2mm}
				\item questions under discussion (issues raised by the dialogue so far); \vspace{-2mm}
				\item moves (dialogue ``history''); \vspace{-2mm}
				\item pending utterance (what is being said but not grounded).
			\end{itemize}
			This is summarized in the following record type, called a Dialogue Game Board (DGB) Type:
			\begin{equation*}
				\texttt{DGBType} =
				\left[
				\begin{array}{l l l}
				\mbox{Speaker}  &:& \texttt{Ind} \\
				\mbox{Addressee} &:& \texttt{Ind} \\
				\mbox{Facts} &:& \langle \texttt{Set}(\texttt{Prop}), \texttt{Set}(\texttt{Prop}) \rangle \\
				\mbox{Visual} &:& \texttt{Situation} \\
				\mbox{QUD}  &:& \texttt{Poset}(\texttt{Questions})\\
				\mbox{Moves}  &:& \texttt{List}(\texttt{LocProp})\\
				\mbox{Pending} &:& \texttt{List}(\texttt{LocProp})
				\end{array}
				\right ]
				\label{dgb_type}
			\end{equation*}
			In the following subsections we will go over the details of the different fields and types that ahve just been introduced in \texttt{DGBType}.
			\subsubsection{Turns: speaker and addressee}
				These fields are the most straightforward; indeed they just keep track of the turn ownership (who speaks and who listens at a given time). ``Speaker'' and ``Addressee'' fields are of type \texttt{Ind}, which can be seen as a subtype of \texttt{Entity}\footnote{type \texttt{e} of Montague grammars}. A simple way to implement that would be to give each participant a unique ID.
			\subsubsection{Facts}
				``Facts'' are assumptions held by the participants. These assumptions allow them to answer questions, to justify themselves, and possibly to start a conversation about a given subject. This structure is presented as a pair of partially ordered sets, a view that differs from \cite{ginzburg2012, ginzburg2017}, where ``Facts'' are seen as ``shared assumptions''. Indeed, conversational events like contradiction come from an asymmetry of belief between participants, and that is why we thought it would be beneficial to store two separate copies of the participants' respective beliefs.\\
				
				Each set of facts, one per participant, is thus a poset of ``propositions'' (type \texttt{Prop}). These propositions can be written in First-Order Logic, but also in other systems (like for instance MRS, cf. Section \ref{RMRS}, or TTR cf. Section \ref{DS_TTR})\footnote{we will finally opt for one of these last alternatives, because the use of type \texttt{Prop} in the field ``Pending'' forces us to have a richer representation that must include underspecification. So this choice is not driven by the nature of ``Facts'', but rather by our will to unify the types for propositions across the different fields, some of them -- like ``Pending'' -- being incremental.}. The partial order would be the implication relation (this choice will be justified in Section \ref{contradiction}).
			\subsubsection{Questions}
				The field ``Questions'' is also a partially ordered set of element of type \texttt{Question}. The order used in that case is a bit more unclear; however it has to account for a type of \textit{precedence} \cite{ginzburg2012}. This means that at a given point of the conversation, a question should be more topical than the others. We could say that it is basically the last question that has been set, but yet not always. This notion of ordering between questions is crucial to establish a notion of relevance in the Gricean terms \cite{grice1989}, as we will see in Section \ref{conversational_rules}.
			\subsubsection{Moves}
				The field Moves is a history of the speech acts that have been grounded. This field  has been introduced by Ginzburg (see for instance \cite{ginzburg2012, ginzburg2020}) and its content relates to situation semantics. Indeed, the objects of type \texttt{LocProp} that are listed in this field are of the form:
				\begin{equation*}
					\texttt{LocProp} = \left[
					\begin{array}{l l l}
					\mbox{Situation} &:& \texttt{Sit} \\
					\mbox{Speaker} &:& \texttt{Ind} \\
					\mbox{Addressee} &:& \texttt{Ind} \\
					\mbox{R} &:& \texttt{SpeechAct} \\ 
					\mbox{Content} &:& \texttt{Prop} \\
					\mbox{Situation type} &:& R(\mbox{Speaker}, \mbox{Addressee}, \mbox{Content}) \\
					\end{array}
					\right].
				\end{equation*}
				The field named ``Situation type'' binds a certain utterance to a certain situation; as we have seen this is the core idea of Austinean semantics. The type called \texttt{SpeechAct} can be seen as the supertype for illocutionary speech acts as defined in \cite{austin1975} : \texttt{Assert} (for ``Assertives''), \texttt{Direct} (for ``Directives''), \texttt{Com} (for ``Commissives''), \texttt{Expr} (for ``Expressives''), \texttt{Decl} (for ``Declarations''). But of course we could imagine many more precise subtypes for each of these illocutionary speech acts. We will use some of them in Section \ref{conversational_rules} too.
			\subsubsection{Pending}
				The field ``Pending'' is quite similar in its structure to the field ``Moves''. Indeed, we will see that utterances tend to be pushed first in the ``Pending'' field, before being moved to the ``Move'' field. The field ``Pending'' is indeed used to store the ungrounded utterances, which includes the forthcoming utterance. This last utterance is a bit particular because, for the sake of incrementality, it must be updated in a word-by-word basis. This also means that this last utterance is most of the time underspecified. This justifies the need for incremental representation of meaning and incremental parsing methods. These topics will be studied more in depth in the next section.
				
				% poesio : discourse situation

			
	\section{Dealing with incrementality}
%	% poesio micro conversatrional events
		In this section, we present the current approaches that tackle the problem of incremental semantic interpretation : Dynamic Syntax with TTR and Incremental Processing (DS-TTR IP) and Incremental Robust Minimal Recursion Semantics (iRMRS). Both tend to comply with a more abstract, high-level framework designed by \cite{schlangen2009} for general incremental systems.
		\subsection{A general model for incremental processing}\label{iu_model}
			A very abstract, general framework for incremental processing in dialogue has been developed by \cite{schlangen2009}. It is inspired by \cite{young1989} token passing architecture. This model is independent of the formalism used for syntactic/semantic modeling, and has many applications in dialogue, from speech recognition to semantic interpretation. The key concepts of this model are \textit{incremental units} (or IUs) and \textit{modules}. Modules basically process incremental units.
			\subsubsection{Incremental Units}
				An IU refers to the smallest possible chunk of information that can be processed independently by the system. IUs are of different types, from raw input (audio signal, character string...) to higher order structures (records as we will see later, or elements of first order logic...). The segmentation of IUs can be temporal (fixed time window) or categorical (word, phoneme etc.) In our case, IUs will basically have the granularity of a word, so they will represent words or their syntactic/semantic counterparts. More generally, IUs carry three types of information:
				\begin{itemize}
					\item a confidence score: how sure is the system about the production of this IU?\vspace{-2mm}
					\item a commitment flag: is it possible to revoke the IU (correction)?\vspace{-2mm}
					\item a history: which module has processed the IU so far?
				\end{itemize}
				Besides this metadata, IUs are defined by the relations they may have with other IUs:
				\begin{itemize}
					\item horizontal relations: between IUs of the same linguistic level (semantic, syntactic...), \textit{i.e.} produced by the same module. This includes the \textit{successor} relation that keeps track of linear order in which the IUs have been produced (alternative paths being possible). One can also think of other non-linear dependency constraints (modification, specification, $\theta$-roles...); \vspace{-2mm}
					\item vertical relations: these are hierarchical relations, between IUs of different linguistic levels. This includes most importantly the \textit{grounded} relation, which links an higher order IUs to the lower order IUs that justify its production.
				\end{itemize}
			\subsubsection{Modules}
				Modules are processing units. A module consumes IUs and produces IUs; it consists of three elements:
				\begin{itemize}
					\item a left (input) buffer: where IU arrive before being processed;\vspace{-2mm}
					\item a right (output) buffer: where IU exit after being processed; \vspace{-2mm}
					\item a processor, that turns a bunch of input IUs to a bunch of output IUs
				\end{itemize}
				Since the system is not necessarily instantaneous, a processor may sometimes ``wait'' for more than one IU in the left buffer to produce a single output IU. It is thus a function from sets of IUs to sets of IUs, and not just a function from IUs to IUs. This is depicted in Figure \ref{fig:module_iu}\\
				
				\begin{figure}[h]
					\centering
					\begin{tabular}{|c|r|c|l|}
						\hline
						& Left buffer & Processor & Right buffer\\
						$t_0$ & $\underline{\blacksquare}$ & & \\
						$t_1$ & $\underline{\blacksquare \blacksquare}$ & $\square \square \mapsto \ocircle$ & $\CIRCLE$ \\
						$t_2$ & $\underline{\blacksquare \blacksquare \blacksquare}$ &  $\square  \square \square \mapsto \triangle$ & $\otimes \blacktriangle$ \\
						$t_5$ &
						$\underline{\blacksquare} \blacksquare \blacksquare \blacksquare$ &
						$\left | \begin{tabular}{c}
							$\square \mapsto  \ocircle$ \\
							$\square \mapsto \triangle$ \\
						\end{tabular}\right.$
						& $\blacktriangle < \begin{tabular}{c}
						$\CIRCLE$ \\
						$\blacktriangle$
						\end{tabular}$ \\
						\hline
					\end{tabular}
					\caption{Schematic of a module (incremental processing unit) with delayed, batch computation ($t_1$), revision ($t_2$), and production of alternatives ($t_3$)}
					\label{fig:module_iu}
				\end{figure}
				
				
				When IUs flow through the system, they go from one module to another. Modules thus form a particular kind of graph. The edges of this graph allow IUs to circulate through modules, with several potential restrictions:
				\begin{itemize}
					\item directions: edges are oriented; they may or may not be bidirectional;\vspace{-2mm}
					\item multimodality: edges can allows for multimodal input  (IUs of different types) or output;\vspace{-2mm}
					\item filtering: edges may filter certain IUs depending on their syntactic/semantic characteristics
				\end{itemize}
				
				The two approaches we will mention in the following section have been implemented within the Incremental Unit framework. Moreover, both of them rely on record types.
			
			
			
			
			
		

		
		\subsection{DS-TTR with incremental processing} \label{DS_TTR}
			Dynamic syntax \cite{kempson2001} is a model that incorporates syntactic and semantic parsing. Indeed, DS parsing generates the syntactic tree and decorates it with typed $\lambda$-terms, in one pass. In DS-TTR \cite{hough2011,hough2015}, the $\lambda$-terms have enriched types:
			\begin{itemize}
				\item a record $r$ is a term in DS-TTR;\vspace{-2mm}
				\item a function from a record to a term $t$ of DS-TTR ($\lambda r. t$) is also a term of DS-TTR.
			\end{itemize}
			Besides, DS relies on a lexicon that is composed of actions. These actions are formulated as IF-THEN-ELSE statements that enrich monotonically the content of the semantic tree (completion of an existing node or creation of a new node). Actions are of two kinds:
			\begin{itemize}
				\item lexical actions (triggered when a new word is uttered); \vspace{-2mm}
				\item computational actions (that can be used to enrich the semantic tree depending on the words that are already present).
			\end{itemize}
			A DS parse is then a series of lexical actions with possibly intermediate computational actions:
			\begin{equation*}
				(LEX \ COMP^*)^* 
			\end{equation*}
			Each action is performed according to a pointer that signals the node that is currently under construction. This node is the one that is evaluated when checking the preconditions in the IF clause of an action. The progression of the pointer is bottom-up.

		\subsection{Incremental RMRS}\label{RMRS}
			Minimal Recursion Semantics \cite{copestake2005} is a model that is characterized by a flat semantics, whose central element is the elementary prediction (EP). As we will see, an utterance (type \texttt{Utt}) is modeled as a bag of predications, a top and a local top handle, and a list of constraints (cf. Figure \ref{fig:mrs_types}).
			\subsubsection{Predications}
				Predications are handle-predicate pairs. Handles (in that situation they are also called \textit{labels}) are indices that allow to keep track of the original tree structure on which the MRS has been built\footnote{thus this structure is obtained after syntactic parsing}. They denote node addresses in the original tree; and if two EPs have the same handle, they must be conjuncted. Predicates can have two kinds of possible arguments:
				\begin{itemize}
					\item ordinary variables like x, y, z... modeled as a list of elements of primary type \texttt{Var}; \vspace{-2mm}
					\item handles that refer to the EP's daughters in the underlying tree. Handles seen as arguments (also called \textit{holes}) thus reify the branches of the tree and flatten the structure (predicates no longer apply to other predicates...). Holes are naturally modeled as a list of handles (of type \texttt{Handle}).
				\end{itemize}
				In so doing, predicates can be represented as elements of type \texttt{Pred}, see Figure \ref{fig:mrs_types}.
				\begin{figure}[h]
					\begin{equation*}
						\centering
						\begin{array}{ccc}
						\texttt{Utt} = & \texttt{Pred} = & \texttt{QEQ} = \\
						\left[
						\begin{array}{l l l}
						\mbox{Hook} &:& \left[
						\begin{array}{l l l}
						\mbox{GTOP} &:& \texttt{Handle}\\
						\mbox{LTOP} &:& \texttt{Handle}\\
						\end{array}
						\right]\\
						\mbox{Rels} &:& \texttt{Bag}(\texttt{Pred})\\
						\mbox{Cons} &:& \texttt{List}(\texttt{QEQ})\\
						\end{array}
						\right]
						&
						\left[
						\begin{array}{lll}
						\mbox{Rel} &:& \texttt{Prop}\\
						\mbox{Label} &:& \texttt{Handle} \\
						\mbox{Args} &:& \texttt{List}(\texttt{Var}) \\
						\mbox{Holes} &:& \texttt{List}(\texttt{Handle}) \\
						\end{array}
						\right]
						&
						\left[
						\begin{array}{lll}
						\mbox{HARG} &:& \texttt{Handle}\\
						\mbox{LARG} &:& \texttt{Handle}
						\end{array}
						\right]\\
						\end{array}
						\label{mrs_record}
					\end{equation*}
					\caption{Main types in RMRS}
					\label{fig:mrs_types}
				\end{figure}
				
				
				In that setting, generalized quantifiers have for instance one variable (the one that is bound by this quantifier) and two holes (one that refer to the restriction predicates, and one that refers to the body predicates). Modifiers like adverbs usually have only one hole (that refers to the predicate they modify) and no variables. And ``regular'' predicates (like verbs) usually have only variables.\\
				
				Note that handles also allow for underspecification and ambiguity, which makes RMRS more attractive for our purpose than simple predicate logic. An underspecified handle \footnote{\textit{i.e.} a handle that is not addressed and that is passed to a predication} may indeed generate various syntactic trees. This is exemplified in Figure \ref{fig:handle_underspec}. This mechanism is also used for the modeling of Wh-questions, which is known to be a rather complex problem (see for instance \cite{egg1998} for a quite complete account of questions and their interactions with quantifiers).
			\subsubsection{Constraints, (local) top handle}
				Conversely, underspecification can be resolved by adding constraints to the structure. Constraints are separate equations between handles; basically, these equations allow:
				\begin{itemize}
					\item to ``reconnect'' the underlying tree, when a vacuous hole is equated with a label; \vspace{-2mm}
					\item to conjunct daughters, when two labels are equated; \vspace{-2mm}
					\item equating the top handle with a label.
				\end{itemize}
				Constraints are added incrementally while parsing a sentence (and thus merging constituents). However in that case, a special, more relaxed kind of equality is used: equality modulo quantifier ($=_{qeq}$). A hole is qeq to a label ($h=_{qeq}l$) iff:
				\begin{itemize}
					\item they are equal ($h = l$); \vspace{-2mm}
					\item or the hole refers to the label of a quantifier and that quantifier's body contains another hole $h'$ such that $h'=_{qeq} l$.
				\end{itemize}
				In other words, a right traversal of the tree from the hole to the label shall only cross quantifier nodes. The relaxed equality allows to keep scopal ambiguity when necessary.\\
				Parsing involves constraints, but also involves headedness, like in traditional syntactic theories including HPSG. In RMRS, the notion of headedness is embodied by the local top feature which is the topmost label that is not a quantifier label. When two constituents are merged, the local top of the head is preserved (cf. the three syntactic rules of Figure). 
				
				
				
				
			%\subsubsection{The \texttt{SemRec} type}
		%	The type of semantic objects is (\texttt{Lambda})\texttt{SemRec}. It is based on Minimal Recursion Semantics (MRS) \cite{copestake2005}. MRS is very attractive, because it models expressions of first order logic with generalized quantifiers, and allows for underspecification (scope or reference).
			
		%	Figure \ref{fig:tree_mrs} shows how FOL-GQ is translated into MRS decorated trees\footnote{the interpretation fucntion $\llbracket . \rrbracket^h$ turns a FOL expression into a tree and gives the root the label $h$} that capture scope informations. Scope ambiguity can be introduced by simply ``disconnecting'' the right branch of quantified nodes. Such a vacuous branch is called a \textit{hole}.\\
		%	Since each node's position in the tree is encoded in its \textit{handles} (its own handle, also called \textit{label}, and the handles it points to), there is no need to keep the tree representation, and it becomes possible to pack all the nodes together in a record, as represented in Figure \ref{mrs_record}. Note that the record of this figure has three main fields :
		%	\begin{itemize}
		%		\item Rels sums up the informations available in a MRS tree structure. It is a bag of nodes, each node coming with a label and a predicate / binder that is applied to variables (bound or free) or to other relations (represented by their label).
		%		\item Hook is used to combine properly two MRS structures and to deal with reference resolution. The local top of an expression is the predicate that has the widest scope.
		%		\item Cons restricts the possible combinations (i.e. how to ``plug'' back in holes by equating handles)in the case of underspecified MRS. Each constraint is represented by a qeq relation between an ``upper'' argument handle h and a ``lower'' label l, connected by a chain of nodes.
		%	\end{itemize}
				
	\section{Updating DGBs} \label{conversational_rules}
		Models such as Dynamic Syntax and Minimal Recursion Semantics build semantic content monotonically and incrementally. The question is then how to use this content to update the current dialogue state in a relevant way. In this section we devise higher-level operations that model sub-sentential phenomena that are typical of dialogue: acknowledgement, clarification request, repair, cooperation, ellipsis.
		\subsection{Notations}
			We will define updates as programs from DGBs to DGBs. An update occurs when the input DGB satisfies some preconditions. The output DGB is then produced according to some postconditions (as we will see, the operation that is performed is always of the same kind). Preconditions and postconditions are typing conditions, that are represented using record types.\\
			
			A record R satisfies the preconditions P iff R is a subtype of P, which means that R contains at least the labels specified in P, and for these labels, their types in R must be subtypes of their types in P. The postconditions Q are automatically verified, because the operation that performed on the input record type is:
			\begin{equation*}
				\lambda R. R \ \boxed{\wedge} \ Q
			\end{equation*}
			Where $\boxed{\wedge}$ is the asymmetric merge operation that produces a record type by performing the following updates:
			\begin{itemize}
				\item copy the labels that are only in R;\vspace{-2mm}
				\item copy the labels that are only in Q; \vspace{-2mm}
				\item for all label that is both in R and Q: \vspace{-2mm}
				\begin{itemize}
					\item if the label is typed with a simple type, keep the type specified in Q;\vspace{-1mm}
					\item if the label is typed with a record type, take the asymmetric merge of R's version and Q's version.
				\end{itemize}
			\end{itemize}
			This ensures that all the labels in Q are present in the output with the right type (the one specified in Q), and thus that the output record type is a subtype of Q (see Appendix \ref{proof_wedge} for details). This also means that the update operation creates monotonic refinements of the initial DGB.
		\subsection{Question}
			\cite{ginzburg2017}
			question q should not be resolvable with facts
			\begin{equation*}
				\begin{recenv}
					Pre &=& \begin{recenv}
						QUD = Q &:& \\
						Moves = Ask(spkr, addr, q) :: M &:& IllocProp
					\end{recenv}\\
					Post &=& \begin{recenv}
						QUD = q :: Q &:& \\ 
					\end{recenv}
				\end{recenv}
			\end{equation*}
		\subsection{Assertion}
			\begin{equation*}
				\begin{recenv}
					Pre  &=& \begin{recenv}
					QUD = Q &:& \\
						Moves = Assert(spkr, addr, p) :: M &:& IllocProp
					\end{recenv}\\
					Post &=& \begin{recenv}
						QUD = p? :: Q &:& 
					\end{recenv}
				\end{recenv}
			\end{equation*}
		% p? = {p, neg p}
		\subsection{Acknowledgement}
			Acknowledgement may be triggered when one speaker approves what the other speaker has just said or was about to say. In the last case, the approver has been able to mentally complete what has been said orally by the other speaker. Moreover, the fact that is acknowledged must be consistent with the other facts the approver believes in. It may or may not be part of the approver's initial  beliefs.

			\begin{sbcquote}{SBC031}
				BETH:&	... But,\\
				&you know,\\
				&... like,\\
				&... she know=s,\\
				&so much about it.\\
				&You know?\\
				&... I said probably that one day the --\\
				&... The teacher and you both just had a bad day,\\
				&and it came off $<$X on X$>$.\\
				SHERRY:&	... Yeah.\\
			\end{sbcquote}
			%sbc031
			\begin{equation*}
			\begin{recenv}
			Pre  &=& \begin{recenv}
			Facts = F\\
			QUD = Q' :: p? :: Q &:& \\
			Moves = Ack(spkr, addr, p) :: M &:& IllocProp
			\end{recenv}\\
			Post &=& \begin{recenv}
			Facts = p :: F\\
			QUD = resolve(Q' :: Q, p) &:& 
			\end{recenv}
			\end{recenv}
			\end{equation*}
		\subsection{Contradiction} \label{contradiction}
			\begin{equation*}
			\begin{recenv}
			Pre  &=& \begin{recenv}
			Facts = F\\
			QUD = Q' :: p? :: Q &:& \\
			Moves = Assert(spkr, addr, not(p)) :: M &:& IllocProp
			\end{recenv}\\
			Post &=& \begin{recenv}
			QUD = Q' :: not(p)? :: Q &:& 
			\end{recenv}
			\end{recenv}
			\end{equation*}
			
		\subsection{Grounding}
			\begin{equation*}
			\begin{recenv}
			Pre  &=& \begin{recenv}
			Moves = M &:& List(LocProp) \\
			Pending = p :: P &:& List(LocProp)\\
			\end{recenv}\\
			Post &=& \begin{recenv}
			Moves = p :: M &:& List(LocProp)\\
			Pending = P &:& List(LocProp)
			\end{recenv}
			\end{recenv}
			\end{equation*}
		\subsection{Clarification request}
			\begin{equation*}
			\begin{recenv}
			Pre  &=& \begin{recenv}
			Moves = M &:& List(LocProp) \\
			Pending = p :: P &:& List(LocProp)\\
			\end{recenv}\\
			Post &=& \begin{recenv}
			Moves = Ask(spkr, addr, CQ(p)) :: M &:& List(LocProp)\\
			Pending = p :: P &:& List(LocProp)
			\end{recenv}
			\end{recenv}
			\end{equation*}
			$CQ$ is a function from assertive propositions to questions. It focuses on one element of the proposition (an entity, or a predicate) and asks about this element. Therefore, it complies with the so-called Reprise Content Hypothesis:
			\begin{center}
				A fragment reprise question queries exactly the standard semantic content of the fragment being reprised.
			\end{center}
			Basically, CQ will p abstract over a variable or a predicate. For example:
			
			\begin{sbcquote}{SBC042}
				CURT:&	... Do you remember the uh,\\
				&... X program on TV,\\
				&.. about hunter virus,\\
				KITTY:&	... About what virus?\\
				CURT:&	.. Hunter virus,\\
				KITTY:&	... Hutter.\\
				&... What's that about.\\
				&I don't re- I don't remember.\\
			\end{sbcquote}
		\subsection{Self Repair}
			\begin{equation*}
			\begin{recenv}
			Pre  &=& \begin{recenv}
			Pending = p :: P &:& List(LocProp)\\
			\end{recenv}\\
			Post &=& \begin{recenv}
			Pending = repair(p) :: P &:& List(LocProp)
			\end{recenv}
			\end{recenv}
			\end{equation*}
			\begin{sbcquote}{SBC022}
				LANCE:&	For the most part,\\
				&... I mean I was able to k- --\\
				&I think I was able to keep up with that stuff,\\
				RANDY:&	With the ARTS.\\
				LANCE:&	.. Yeah.\\
			\end{sbcquote}

		\subsection{Cooperation}
			\begin{equation*}
			\begin{recenv}
			Pre  &=& \begin{recenv}
			Speaker = i\\
			Addressee = j\\
			Pending = p :: P &:& List(LocProp)\\
			\end{recenv}\\
			Post &=& \begin{recenv}
			Speaker = j\\
			Addressee = i\\
			Pending = predict_j(p) :: P &:& List(LocProp)
			\end{recenv}
			\end{recenv}
			\end{equation*}
		\subsection{Ellipsis}



\newpage
\bibliographystyle{apalike}
\bibliography{bibliography}
\newpage
\begin{appendices}
	\section{Figures of Section 1}
		\subsection{The attachment problem : a case of structural ambiguity}
			\begin{figure*}[h]
				\centering
				\begin{subfigure}[t]{\textwidth}
					\Tree [.ROOT
					[.S
					[.NP [.PRP I ] ]
					[.VP [.VBP see ]
					[.NP
					[.NP [.DT the ] [.NN man ] ]
					[.PP [.IN with ]
					[.NP [.DT a ] [.NN telescope ] ] ] ] ] ] ]
					\subcaption{the PP seen as a complement of the NP}
				\end{subfigure}
				
				\begin{subfigure}[t]{\textwidth}
					\Tree [.ROOT
					[.S
					[.NP [.PRP I ] ]
					[.VP [.VBP see ]
					[.NP [.DT the ] [.NN man ] ]
					[.PP [.IN with ]
					[.NP [.DT a ] [.NN telescope ] ] ] ] ] ]
					\caption{the PP seen as a complement of the VP}
				\end{subfigure}
			\caption{The PP-attachment problem, generating two ambiguous trees (built using the online version of the \href{http://nlp.stanford.edu:8080/parser/}{Stanford Parser})}
			\label{fig:att_pb}
			\end{figure*}

	\section{Figures of Section 3}
		\subsection{RMRS: from tree to bag}
			\begin{figure*}[h]
				\centering
				\begin{subfigure}[t]{.3\textwidth}
					\Tree [.{every(x)} [.{man(x)} ] [.{mortal(x)} ] ]
				\subcaption{Original semantic tree}
				\end{subfigure}
				\begin{subfigure}[t]{.3\textwidth}
					\Tree [.{h_0 : every(x)} [.{h_1 : man(x)} ] [.{h_2 : mortal(x)} ] ]
				\subcaption{Adding addresses}
				\end{subfigure}
				\begin{subfigure}[t]{.3\textwidth}
					\Tree [.{h_0 : every(x, h_1, h_2)} [.{h_1 : man(x)} ] [.{h_2 : mortal(x)} ] ]
				\subcaption{Passing addresses}
				\end{subfigure}
				\caption{From tree structure to flat representation $\lbrace h_0 : every(x, h_1, h_2), h_1 : man(x), h_2 : mortal(x)\rbrace$}
				\label{fig:rmrs}
				\end{figure*}
			\subsection{RMRS : from FOL to RMRS trees}
				\begin{figure}[h]
					\centering
					\begin{subfigure}[t]{.4\textwidth}
						\centering
						$\llbracket P(x) \rrbracket^h$, with $P$ atomic\vspace{5mm}
						\hrule
						\vspace{5mm}
						h : P(x)
					\end{subfigure}
					\begin{subfigure}[t]{.4\textwidth}
						\centering
						$\llbracket \exists/\forall x : P(x), \ Q(x) \rrbracket^{h}$\vspace{5mm}
						\hrule \vspace{5mm}
						\Tree [.{h : some/every(x, h$_{restr}$, h$_{body}$)} [.{$\llbracket P(x) \rrbracket^{h_{restr}}$}  ] [.{$\llbracket Q(x) \rrbracket^{h_{body}}$}  ] ]
					\end{subfigure}
				
					\vspace{8mm}
					\begin{subfigure}[t]{.4\textwidth}
						\centering
						$\llbracket\neg P(x)\rrbracket^h$\vspace{5mm}
						\hrule \vspace{5mm}
						\Tree [.{h : not(h')} {$\llbracket P(x) \rrbracket^{h'}$} ]
					\end{subfigure}
					\begin{subfigure}[t]{.4\textwidth}
						\centering
						$\llbracket P(x) \wedge Q(x) \rrbracket^h$\vspace{5mm}
						\hrule \vspace{5mm}
						\Tree [.{} {$\llbracket P(x) \rrbracket^h, \llbracket Q(x) \rrbracket^h$} ]
					\end{subfigure}
					\caption{Translation of FOL atomic formulae into MRS tree structures with handles}
					\label{fig:tree_mrs}
				\end{figure}
			
			
			\subsection{RMRS: underspecification and scope ambiguity}
				\begin{figure*}[h]
					\centering
					\begin{subfigure}[t]{.45\textwidth}
						\Tree [.{h_0 : every(x, h_1, \textbf{h_2})} [.{h_1 : man(x)} ] [.{h_2 : exists(y, h_3, \textbf{h_4})} [.{h_3 : woman(y)} ] [.{h_4 : love(x, y)} ] ] ]
						\subcaption{Universal with wider scope ($h_{2, 4} = h_2$ and $h_{0, 4} = h_4$)}
					\end{subfigure}
					\begin{subfigure}[t]{.45\textwidth}
						\Tree [.{h_2 : exists(y, h_3, \textbf{h_0})} [.{h_3 : woman(y)} ] [.{h_0 : every(x, h_1, \textbf{h_4})} [.{h_1 : man(x)} ] [.{h_4 : love(x, y)} ] ] ]
						\subcaption{Existential with wider scope ($h_{2, 4} = h_4$ and $h_{0, 4} = h_0$)}
					\end{subfigure}
					\par\bigskip
					\begin{subfigure}[t]{.45\textwidth}
						\Tree [.{h_0 : every(x, h_1, h_{2, 4})} [.{h_1 : man(x)} ] [.{h_{2, 4}$\downarrow$} ] ]
						\Tree [.{h_2 : exists(y, h_3, h_{0, 4})} [.{h_3 : woman(y)} ] [.{h_{0, 4}$\downarrow$} ] ]
						\subcaption{Underspecified scope (vacuous branches signaled by $\downarrow$)}
					\end{subfigure}
					\caption{How handles allow for scope ambiguity (bold handles = scopal handles that can be underspecified)}
					\label{fig:handle_underspec}
				\end{figure*}
			\subsection{RMRS: combination rules}
				\subsubsection{Intersective combination}
				\begin{eqnarray*}
					\mbox{LEFT DAUGHTER} &=&
					\left[
					\begin{array}{lll}
						Hook &=& H\\
						Rels &=& R_1\\
						Cons &=& C_1
					\end{array}
					\right]\\
					\mbox{RIGHT DAUGHTER} &=&
					\left[
					\begin{array}{lll}
					Hook &=& H\\
					Rels &=& R_2\\
					Cons &=& C_2
					\end{array}
					\right]\\
					\mbox{MOTHER} &=& 
					\left[
					\begin{array}{lll}
					Hook &=& H\\
					Rels &=& R_1+R_2\\
					Cons &=& C_1+C_2
					\end{array}
					\right]
				\end{eqnarray*}
			\subsubsection{Scopal combination}
					\begin{eqnarray*}
					\mbox{SCOPAL DAUGHTER} &=&
					\left[
					\begin{array}{lll}
					Hook &=& \left[\begin{array}{lll}
						GTop &=& G\\
						LTop &=& L_s
					\end{array}\right]\vspace{2mm} \\ 
					Rels &=& R_s = R_{s'}+\left[\begin{array}{lll}
					Label &=& L_s\\
					\dots\\
					Hole &=& h\\
					\dots
					\end{array}\right]\\
					Cons &=& C_s
					\end{array}
					\right]\\
					\mbox{NON-SCOPAL DAUGHTER} &=&
					\left[
					\begin{array}{lll}
					Hook &=& \left[\begin{array}{lll}
					GTop &=& G\\
					LTop &=& L_{ns}
					\end{array}\right]\vspace{2mm} \\
					Rels &=& R_{ns}\\
					Cons &=& C_{ns}
					\end{array}
					\right]\\
					\mbox{MOTHER} &=& 
					\left[
					\begin{array}{lll}
					Hook &=& \left[\begin{array}{lll}
					GTop &=& G\\
					LTop &=& L_s
					\end{array}\right] \vspace{2mm} \\
					Rels &=& R_s+R_{ns}\\
					Cons &=& C_s+C_{ns}+\lbrace h =_{qeq} L_{ns}\rbrace
					\end{array}
					\right]
				\end{eqnarray*}
			\subsubsection{Root condition}
				\begin{equation*}
					ROOT = \begin{recenv}
						Hook = \begin{recenv}
							GTop = G_0 \\
							LTop = L_0
						\end{recenv} \vspace{2mm} \\
						Rels = R \\
						Cons = C_0 :: G_O =_{qeq} L_0 :: C_0'
					\end{recenv}
				\end{equation*}
			\subsection{Proof: $\lambda R. R \ \boxed{\wedge} \ Q$ produces a subtype of Q}
				In this section we will prove that if a record type R is such that $\exists P, \ R = P \boxed{\wedge} Q$, then $R \leq : Q$. The subtyping relation $R \leq : Q$ means that $labels(Q) \subseteq labels(R)$ and:
				\begin{equation*}
					\forall l \in labels(Q)
					\implies
					\left\lbrace\begin{array}{lll}
						l :_{Q} T_{s} &\implies& l :_{R} T_{s}\\
						l :_Q T_{r} \wedge l :_R T_r'&\implies& T_r' \leq: T_r
					\end{array}\right.
				\end{equation*}
				Where $T_s$ is a simple type, and $T_r, T_r'$ are record types.\\
				We prove this result by strong induction on the maximal level of record type embedding in R.\\
				\underline{Base case:} no embedding (i.e. all labels have simple types)
				\begin{equation*}
					R = P \ \boxed{\wedge} \ Q = \left[\begin{array}{lll}
						l_1 &:& T_1\\
						\dots\\
						l_n &:& T_n
					\end{array}\right]
				\end{equation*}
				First, $labels(Q) \subseteq labels(R)$. Indeed, let $l \in labels(Q)$
				\begin{itemize}
					\item if $l \notin labels(P)$, then rule \boxed{2} ensures that $l$ is copied in R
					\item in $l \in labels(P)$ then rule \boxed{3.1} ensures that $l$ is copied in R
				\end{itemize}
				Now, let $l \in labels(Q)$ and suppose that $l :_{Q} T_s$. Let us show that $l :_{R} T_s$.
				\begin{itemize}
					\item if $l \notin labels(P)$, then rule \boxed{2} ensures that $l$ is copied in R, so $l :_{R} T_s$
					\item if $l \in labels(P)$, then rule \boxed{3.1} ensures that Q's version of $l$ in copied in R, so $l :_R T_s$.
				\end{itemize}
				That proves our property for the base case.\\
				\underline{Inductive case:} suppose the property is verified for all level of embedding in $[0;n]$. Let us take R with a maximal level of embedding $n+1$:
				\begin{equation*}
					R = P \ \boxed{\wedge} \ Q = \left[\begin{array}{lll}
					l_1 &:& T_1\\
					\dots\\
					l_n &:& T_n
					\end{array}\right]
				\end{equation*}
				For all $i \in [1; n]$, the maximal level of embedding of $T_i$ is $n$, so $\exists P', \ T_i = P' \ \boxed{\wedge} \ Q' \implies T_i \leq: Q'$ (induction hypothesis).\\
				Let $l \in labels(Q)$:
				\begin{itemize}
					\item if $l \notin labels(P)$, then rule \boxed{2} ensures that $l$ is copied in R, so $l \in labels(R)$, and the copy operation ensures that $l :_{Q} T \iff l :_R T$
					\item if $l \in labels(P)$ and $l :_Q T_s$, then rule \boxed{3.1} ensures that Q's version of $l$ is copied in R, so $l \in labels(R)$ and $l :_Q T \iff l:_R T$ 
					\item if $l \in labels(P)$, $l :_Q T_r$ and $l :_P T_r''$, then rule \boxed{3.2} ensures that $l$ is copied in R, so $l \in labels(R)$, and that $l :_R T_r'' \ \boxed{\wedge} \ T_r$. Since $T_r'' \ \boxed{\wedge} \ T_r$ has a level of embedding that lies in $[0; n]$, we can apply the induction hypothesis and conclude that  $T_r'' \ \boxed{\wedge} \ T_r \triangleq T_r' \leq: T_r$
				\end{itemize}
				That concludes the proof.
			
			
\end{appendices}


\end{document}
 